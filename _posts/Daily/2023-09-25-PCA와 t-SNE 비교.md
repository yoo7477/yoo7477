---
title : "PCA와 t-SNE 비교"
categories :
- DAILY
tag : [ML]
toc: true
toc_sticky: true
toc_label : "목록"
author_profile : false
search: true
use_math: true
---
<br/>

# PCA와 t-SNE 비교


## 1. 개념  
PCA(Principal Component Analysis)와 t-SNE(t-Distributed Stochasitc Neighbor Embedding)은 모두 차원축소(Dimensionality Reduction) 기법임. 차원축소란 매우 많은 feature로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것을 말함. 

## 2. 사용하는 이유
일반적으로 feature가 많을 수록 데이터 간 거리가 기하급수적으로 증가하기 때문에 sparse 구조를 가지게 되어 모델의 신뢰도가 떨어지는 것을 방지하기 위함.
  
## 3. 계산방법
PCA는 [이전 post 참고.](https://yoo7477.github.io/) t-SNE는 고차원의 데이터를 2차원의 embedding vector를 학습함으로써, 2차원의 지도로 표현함. 때문에 vector visualiztion을 위해 자주 이용됨. t-SNE는 고차원 공간에서 유사한 두 벡터가 2차원 공간에서도 유사하도록(데이터간 neighbor structure를 보존하도록) 원 공간에서의 점들 간 유사도를 보존하면서 차원을 축소함.

절차
---
1) 고차원의 원 공간에서 데이터 간 유사도$p_{ij}$를 정의함.
2) 저차원의 임베딩(축소) 공간에서 데이터 간 유사도$q_{ij}$를 정의함.
3) 저차원의 공간이 고차원 공간에 가까워지도록 gradient descent로 계산해 데이터를 변환함.
---
1) 고차원의 원 공간에서 데이터 간 유사도$p_{ij}$를 정의함.
   $p_{ij}$를 정의하기 위해 점 $x_i$에서 $x_j$로의 유사도인 $p_{j|i}$를 정의해야함. 그래서 먼저 기준점 $x_i$에서 다른 모든 점들과의 유클리드 거리 $|x_i-x_j|$를 계산함. 그리고 이 거리를 기반으로 기준 점과 다른 점들 간의 거리가 얼마나 가까운지를 확률로 나타냄. 확률로 나타내는 방법은 점과 점의 거리를 $\sigma_i$로 나누고 negative expoential을 취함=$\exp(-|x_i-x_j|^2/2\sigma_i^2)$. 그리고 모든 점들과의 거리의 합=$\exp(-|x_i-x_k|^2/2\sigma_i^2)$으로 각각을 나눠주면 확률 형식이 됨. 

   $p_{j|i}$=$\frac{\exp(-|x_i-x_j|^2/2\sigma_i^2)}{\sum_{k\ne i}\exp(-|x_i-x_k|^2/2\sigma_i^2)}$


   여기서 $\sigma_i$는 모든 점마다 다르게 정의 되는데 t-SNE가 안정적인 학습 결과를 가지게 되는 부분임. 결과적으로 점과 점사이의 거리가 멀어서 분모가 작아지게 되면 $p_{j|i}$도 작아지게 되고 점과 점사이가 가까우게 되면 유사도가 커져서 유사도가 커지는 방향으로 점들이 이동하는 모델임. 






## 4. 파이썬 코드 구현

## 5. 생각해보기
### Q1. 

## 참고
[t-SNE origin, 2008](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)  

[t-SNE 관련 web blog1](https://lovit.github.io/nlp/representation/2018/09/28/tsne/)

[t-SNE 관련 web blog2](https://3months.tistory.com/571)

[t-SNE 관련 web blog3](https://aaweg-i.medium.com/pca-vs-t-sne-dimensionality-reduction-techniques-fdd7908973a4)


